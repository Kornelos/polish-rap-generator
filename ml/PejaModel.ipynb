{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PejaModel",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP21Q6X5U38Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "68f54a78-64cc-412e-9486-016141ab3104"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.27.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (45.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmXVU-wtHo4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('peja.csv')\n",
        "lst = df[\"Text\"].to_numpy().tolist()\n",
        "text = \" \".join(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gie_8E2x3FhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "01484358-936c-446f-8339-4fcc6a0c1f57"
      },
      "source": [
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "# TODO: import data from scraper\n",
        "\n",
        "# text = 'placeholder for real song text placeholder for real song text xD'\n",
        "words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "print(\"Text is {} words long\".format(len(words)))\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print ('There are {} unique characters'.format(len(vocab)))\n",
        "char2int = {c:i for i, c in enumerate(vocab)}\n",
        "int2char = np.array(vocab)\n",
        "# print('Vector:\\n')\n",
        "# for char,_ in zip(char2int, range(len(vocab))):\n",
        "#     print(' {:4s}: {:3d},'.format(repr(char), char2int[char]))\n",
        "\n",
        "text_as_int = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
        "print ('{}\\n mapped to integers:\\n {}'.format(repr(text[:100]), text_as_int[:100]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.0.0-alpha0\n",
            "Text is 136595 words long\n",
            "There are 114 unique characters\n",
            "'\\nTekst piosenki:\\r\\n\\r\\n\\t\\t    \\t\\t    \\r\\n                    A ja robię ten szmal, mam tu muzę, mam tu bal\\n'\n",
            " mapped to integers:\n",
            " [ 1 50 64 70 78 79  3 75 68 74 78 64 73 70 68 26  2  1  2  1  0  0  3  3\n",
            "  3  3  0  0  3  3  3  3  2  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
            "  3  3  3  3  3  3 31  3 69 60  3 77 74 61 68 96  3 79 64 73  3 78 85 72\n",
            " 60 71 12  3 72 60 72  3 79 80  3 72 80 85 96 12  3 72 60 72  3 79 80  3\n",
            " 61 60 71  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBFzVrhrBMP5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b8dd2a6-ef0e-4721-c10c-2b1f94f0faa8"
      },
      "source": [
        "# Create training and validation data from the text\n",
        "# (make sure the training piece is divisible by the batch size, 64 in this case)\n",
        "tr_text = text_as_int[:704000] \n",
        "val_text = text_as_int[704000:] \n",
        "print(text_as_int.shape, tr_text.shape, val_text.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(936423,) (704000,) (232423,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YVtDsmf6MoH",
        "colab_type": "text"
      },
      "source": [
        "## Building the model\n",
        "### Tunable variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK9I2qxZ6S5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "embedding_dim = 256\n",
        "epochs = 50\n",
        "seq_length = 200\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "#lr = 0.001 #will use default for Adam optimizer\n",
        "rnn_units = 1024\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7qQdvDo-JEe",
        "colab_type": "text"
      },
      "source": [
        "### Training an validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6qF5lxb-MdH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b85c78f6-7899-4a35-def3-ed285e7f8226"
      },
      "source": [
        "tr_char_dataset = tf.data.Dataset.from_tensor_slices(tr_text)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices(val_text)\n",
        "tr_sequences = tr_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "val_sequences = val_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "tr_dataset = tr_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "val_dataset = val_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "print(tr_dataset, val_dataset)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int32, tf.int32)> <BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h-etoad-Wn5",
        "colab_type": "text"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRDtAxKZ-Z3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c891621f-d90f-4bce-bb17-a51547fee6a4"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "     model = tf.keras.Sequential([\n",
        "     tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "     batch_input_shape=[batch_size, None]),\n",
        "     tf.keras.layers.Dropout(0.2),\n",
        "     tf.keras.layers.LSTM(rnn_units,\n",
        "     return_sequences=True,\n",
        "     stateful=True,\n",
        "     recurrent_initializer='glorot_uniform'),\n",
        "     tf.keras.layers.Dropout(0.2), \n",
        "     tf.keras.layers.LSTM(rnn_units,\n",
        "     return_sequences=True,\n",
        "     stateful=True,\n",
        "     recurrent_initializer='glorot_uniform'),\n",
        "     tf.keras.layers.Dropout(0.2),\n",
        "     tf.keras.layers.Dense(vocab_size)\n",
        " ])\n",
        "     return model\n",
        "\n",
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=batch_size)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f7f107c1550>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f7f1056cf60>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O9n9dZ5-_7D",
        "colab_type": "text"
      },
      "source": [
        "### Review the output shape and model, and define loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g-ThE7a-yGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "efbbafc6-63cd-46ce-be92-8613e8b8f36f"
      },
      "source": [
        "model.summary()\n",
        "\n",
        "for input_example_batch, target_example_batch in tr_dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape)\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Loss:      \", example_batch_loss.numpy().mean())\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (64, None, 256)           29184     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (64, None, 256)           0         \n",
            "_________________________________________________________________\n",
            "unified_lstm_6 (UnifiedLSTM) (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "unified_lstm_7 (UnifiedLSTM) (64, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (64, None, 114)           116850    \n",
            "=================================================================\n",
            "Total params: 13,785,714\n",
            "Trainable params: 13,785,714\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(64, 200, 114)\n",
            "Loss:       4.7368875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cXp79uDpgN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "outputId": "c1a448cf-4bff-4587-fb8c-9a544145480b"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "patience = 10\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
        "\n",
        "checkpoint_dir = './checkpoints'+ datetime.datetime.now().strftime(\"_%Y.%m.%d-%H:%M:%S\")\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "history = model.fit(tr_dataset, epochs=epochs, callbacks=[checkpoint_callback, early_stop] , validation_data=val_dataset)\n",
        "print (\"Training stopped as there was no improvement after {} epochs\".format(patience))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 3.4860 - val_loss: 3.2606\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 25s 463ms/step - loss: 3.0241 - val_loss: 2.6716\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 25s 469ms/step - loss: 2.5492 - val_loss: 2.3767\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 25s 472ms/step - loss: 2.3266 - val_loss: 2.1828\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 2.1415 - val_loss: 2.0195\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 1.9958 - val_loss: 1.9057\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 1.8818 - val_loss: 1.8280\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 1.7903 - val_loss: 1.7686\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 1.7095 - val_loss: 1.7289\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 1.6407 - val_loss: 1.6993\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 1.5784 - val_loss: 1.6857\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 1.5146 - val_loss: 1.6768\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 1.4532 - val_loss: 1.6709\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 1.3931 - val_loss: 1.6700\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 1.3314 - val_loss: 1.6750\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 1.2705 - val_loss: 1.6904\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 1.2121 - val_loss: 1.6976\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 1.1562 - val_loss: 1.7158\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 1.0997 - val_loss: 1.7385\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 1.0464 - val_loss: 1.7613\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.9934 - val_loss: 1.7909\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.9436 - val_loss: 1.8280\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.8895 - val_loss: 1.8584\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.8408 - val_loss: 1.8916\n",
            "Training stopped as there was no improvement after 10 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64TQRaNATF9",
        "colab_type": "text"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o1-9HDt-wiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "9189ced7-5caf-481d-c77a-29a3c9d1f800"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "    \n",
        "    print('Generating with seed: \"' + start_string + '\"')\n",
        "  \n",
        "    num_generate = 1000    \n",
        "    input_eval = [char2int[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temperature = 1.0\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(int2char[predicted_id])    \n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=\"Głucha noc\"))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f7f024d6908>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f7f0248fbe0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "Generating with seed: \"Głucha noc\"\n",
            "Głucha noce i dla tego\n",
            "Te kocie ratusze słowa koncert - odważnych\n",
            "recytach te sprzedaż ten to wszystko\n",
            "Warszawia jest w nas, mój gupi się przegaz\n",
            "Wciąż z fawałkami skrudnie i w górze czasu nagrania \n",
            "Wciąż paęarę w to, że kocham i i tam bez bogna?\n",
            "I rozgrawam cały krzachu i Co Co ty jest na drzeżwa\n",
            "Ry jesteś z nami wraz ze słaby. Bólk po prostu i gdzie ma kobiet\n",
            "Jękresz, będzie wiem gdzie tego nie powiem\n",
            "Muże miasto znany scenicznym nę kocham Rycha dotacz\n",
            "Opranalenciściem tylko które nazwać sie mówki na sukcy\n",
            "\t\t    Jarzej - ja rozpustę, i dla wszystkich ludzi \n",
            "rap na blęski zachęcy nabruczałem ja szamste\n",
            "Wieczorem waga wodzę i gówno świat się na wodzie\n",
            "\n",
            "Ja jestem grupi do miniu, od to, nie wierzyłem ciebie \n",
            "Wiel o co chodzi, To mój lat?, o kto solodne gadle\n",
            "Odczuwaj to próbował - czy to się nie dałem \n",
            "odszelu mówisz na bicie bo on to Bysi znak \n",
            "\n",
            "Boistwo sławo i głoszyków często piszę się w tych sercach\n",
            "Gdy mi chuji to nie blefa i i tu na niby do przodo\n",
            "Obrę starej się bogaczy, czujesz si\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dXDlFQqPHTx",
        "colab_type": "text"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CnxBZZVMwkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "790f3774-2b45-4fba-a16e-77effebaa2e9"
      },
      "source": [
        "# print(generate_text(model, start_string=\"Tede to mój ziom\"))\n",
        "\n",
        "model.save('./pejaModel.h5')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating with seed: \"Tede to mój ziom\"\n",
            "Tede to mój ziomek\n",
            "\n",
            "Tylko w życiu mocysz głowę mago. Bierz dobra pęcha\n",
            "Zawsze kam się bardzo poniszczoną kimś będzie teś tak dobrze\n",
            "byłości patrzeć na tym to coś bę nie bała\n",
            "Jestem tylko życie schodzę na pieprzone człowieka\n",
            "Jesteś swoją płatkę w zamieniach ulicznych wojanów,\n",
            "Mamy zarybka krótkiem siedzieć ciągle jeźyce\n",
            "Ale czy się skończyć? poważny na coś zawsze\n",
            "Jestem zwykłym skurwielami tej refytycji, czujesz hip-hop'i komercja\n",
            "Refrezentuję kobietę, spokój ogradę czarną pół\n",
            "W dzisiaj grzieści kręcę, chcę wyrażę się te wszystkie mijasto\n",
            "Dlatego ścianym wlacy za życiam na pewno świata\n",
            "Byst ze mną bezsanności z plastą fozorcjy i skrotnie\n",
            "Wyzwa na 54 Manko Qodro FrLWklaJ \n",
            "Czy chujowe chwila, gdy stariałeś się zachwycę\n",
            "Nie mieć czasu uderzyć na zawsze zmysłonować \n",
            "chcę tego rzeszczego sna granku, zapierdalam na rozlebani \n",
            "Żyjęc razem obiecznego promu wzrok mi ziomków recenzkę\n",
            "Ten chatnie pyska\n",
            "Teraz powiem przykry droga się prodziwa \n",
            "więc będzie taki godny, aspokolam tylko radościa\n",
            "Mówię o co chujem w\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}